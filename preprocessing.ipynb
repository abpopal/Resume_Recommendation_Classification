{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npoi77LUIHVT"
      },
      "source": [
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "yobhAe8JHrvq",
        "outputId": "75e35658-0bd4-4e2a-b07c-9a0dc0df2fb7"
      },
      "source": [
        "import docx2txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0160f88c5db7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docx2txt'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKKRwHdvIgT0"
      },
      "source": [
        "''' Convert Doc to Text '''\n",
        "resume = docx2txt.process('resume.docx')\n",
        "print(resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHoNHBfAIzUn"
      },
      "source": [
        "''' Removing Stop Words from Resume'''\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "filtered_stopwords_resume = remove_stopwords(resume)\n",
        "\n",
        "print(filtered_resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trwbb_DeLEVI"
      },
      "source": [
        "''' Removing all the numbers from Resume '''\n",
        "filtered_numbers_resume = ''.join((item for item in filtered_stopwords_resume if not item.isdigit()))\n",
        "print(filtered_numbers_resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWp0JD1VWGx6"
      },
      "source": [
        "# def remove_single_letters(concept):\n",
        "#     mystring = \"valid\"\n",
        "\n",
        "#     if len(concept) == 1:\n",
        "#         mystring = \"invalid\"\n",
        "\n",
        "#     if len(concept)>1:\n",
        "#         validation = []\n",
        "#         splits = concept.split()\n",
        "#         for item in splits:\n",
        "#             if len(item) > 1:\n",
        "#                 validation.append(\"valid\")\n",
        "#         if len(validation) != 1:\n",
        "#             mystring = \"invalid\"\n",
        "#     return mystring\n",
        "\n",
        "# print(remove_single_letters(filtered_numbers_resume))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLT6ITCU8jr6"
      },
      "source": [
        "stop = set(stopwords.words('english')) #set of stopwords\n",
        "sno = nltk.stem.SnowballStemmer('english') #initialising the snowball stemmer\n",
        "\n",
        "def cleanhtml(resume): #function to clean the word of any html-tags\n",
        "    cleanr = re.compile('<.*?>')\n",
        "    cleantext = re.sub(cleanr, ' ', resume)\n",
        "    return cleantext\n",
        "def cleanpunc(resume): #function to clean the word of any punctuation or special characters\n",
        "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',resume)\n",
        "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
        "    return  cleaned"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2bddHTaLkyH"
      },
      "source": [
        "# filtered_singlestring_resume = ' '.join( [w for w in filtered_numbers_resume.split() if len(w)>1] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM4Odd1uPkQw"
      },
      "source": [
        "''' Removing all the special characters '''\n",
        "filtered_special_resume = re.sub('[^A-z0-9 -]', '', filtered_numbers_resume).lower().replace(\" \", \" \")\n",
        "print(filtered_special_resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eki4ew8gItXf"
      },
      "source": [
        "''' Convert Doc to Text '''\n",
        "job_description = docx2txt.process('jobdescription.docx')\n",
        "print(job_description)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}